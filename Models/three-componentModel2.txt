# marginalised three-component mixture model with in-built dnormmix function

  
model {
  
  # likelihood
  
  for(i in 1:N){
    y[i] ~ dnormmix(mu, prec, mixing_p)

   # inference for z(prob(belongs to component 1|data&parameters))
   z1[i] = (mixing_p[1]*pow(prec[1],0.5)*0.399
          *exp(-0.5*prec[1]*pow(y[i]-mu[1],2)))/
           (mixing_p[1]*pow(prec[1],0.5)*0.399
          *exp(-0.5*prec[1]*pow(y[i]-mu[1],2))+
                (mixing_p[2]*pow(prec[2],0.5)*0.399
                *exp(-0.5*prec[2]*pow(y[i]-mu[2],2)))+
           (mixing_p[3]*pow(prec[3],0.5)*0.399
                *exp(-0.5*prec[3]*pow(y[i]-mu[3],2))))

   # prob(belongs to component 2|data&parameters)
   z2[i] = (mixing_p[2]*pow(prec[2],0.5)*0.399
          *exp(-0.5*prec[2]*pow(y[i]-mu[2],2)))/
           (mixing_p[1]*pow(prec[1],0.5)*0.399
          *exp(-0.5*prec[1]*pow(y[i]-mu[1],2))+
                (mixing_p[2]*pow(prec[2],0.5)*0.399
                *exp(-0.5*prec[2]*pow(y[i]-mu[2],2)))+
           (mixing_p[3]*pow(prec[3],0.5)*0.399
                *exp(-0.5*prec[3]*pow(y[i]-mu[3],2))))

   z[i]=3-2*z1[i]-z2[i]
  }
  
  # prior 
  
  # vague normal prior (centered at zero with small precision)
  # here WLOG we assume mu[2]>mu[1] to give some identifiability
  mu[1] ~ dnorm(0, 1/100)
  mu[2] ~ dnorm(0, 1/100)T(mu[1],)
  mu[3] ~ dnorm(0, 1/100)T(mu[2],)
  
  prec[1] ~ dgamma(0.5, 0.5)
  prec[2] ~ dgamma(0.5, 0.5)
  prec[3] ~ dgamma(0.5, 0.5)

  # monitor sd rather than precision
  sigma[1] = sqrt(1/prec[1])
  sigma[2] = sqrt(1/prec[2])
  sigma[3] = sqrt(1/prec[3])
  
  mixing_p ~ ddirich(c(1.0,1.0,1.0))


   
}
