# marginalised two-component mixture model with zeros trick

data {

  C <- 100000 # This is to ensure that lamda >0

  for(i in 1:N){
       zeros[i] <- 0
    }
  }
  
model {
  
  # likelihood

  for(i in 1:N){
    zeros[i] ~ dpois(lambda[i])
    lambda[i] <- -log(L[i])+C

    c1[i] ~ dnorm(mu[1],prec[1])
    c2[i] ~ dnorm(mu[2],prec[2])
    
    L[i] <- mixing_p[1]*pow(prec[1],0.5)*0.399
          *exp(-0.5*prec[1]*pow(y[i]-mu[1],2))+
                mixing_p[2]*pow(prec[2],0.5)*0.399
                *exp(-0.5*prec[2]*pow(y[i]-mu[2],2))

   # inference for z(prob(belongs to component 1|data&parameters))
   z1[i] = (mixing_p[1]*pow(prec[1],0.5)*0.399
          *exp(-0.5*prec[1]*pow(y[i]-mu[1],2)))/
           (mixing_p[1]*pow(prec[1],0.5)*0.399
          *exp(-0.5*prec[1]*pow(y[i]-mu[1],2))+
                mixing_p[2]*pow(prec[2],0.5)*0.399
                *exp(-0.5*prec[2]*pow(y[i]-mu[2],2)))
   z[i]=-z1[i]+2
    
  }
  
  # prior 
  
  # vague normal prior (centered at zero with small precision)
  # here WLOG we assume mu[2]>mu[1] to give some identifiability
  mu[1] ~ dnorm(0, 1/100)
  mu[2] ~ dnorm(0, 1/100)T(mu[1],)
  
  prec[1] ~ dgamma(0.5, 0.5)
  prec[2] ~ dgamma(0.5, 0.5)
  
  # monitor sd rather than precision
  sigma[1] = sqrt(1/prec[1])
  sigma[2] = sqrt(1/prec[2])
  
  mixing_p ~ ddirich(c(1.0,1.0))
   
}